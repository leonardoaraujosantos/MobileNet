{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imagenet Training MobileNet\n",
    "\n",
    "### References\n",
    "* [Paper](https://arxiv.org/pdf/1704.04861.pdf)\n",
    "* [Other pytrch implementation](https://github.com/marvis/pytorch-mobilenet)\n",
    "* [Training Imagenet with Pytorch](https://github.com/pytorch/examples/tree/master/imagenet)\n",
    "* [Python3 Profiling](https://docs.python.org/3/library/profile.html)\n",
    "* [Profiling pytorch issue I](https://discuss.pytorch.org/t/strange-time-profiling-results-on-gpu/8016)\n",
    "* [Profiling pytorch issue II](https://discuss.pytorch.org/t/profiling-pytorch-scripts/4950/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "# Just some functions to average stuff, and save the model\n",
    "from utils_pytorch import *\n",
    "\n",
    "# Trainning parameters\n",
    "learning_rate = 0.1\n",
    "batch_size = 64\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "workers = 4\n",
    "print_freq = 100\n",
    "epochs = 2\n",
    "#IMAGENET_PATH ='/mnt/eulbh-nas01/qa_analitics/Apical_CNN_training_data/ImageNet/ILSVRC/Data/DET'\n",
    "IMAGENET_PATH = '/home/leoara01/work/IMAGENET/ILSVRC/Data/CLS-LOC'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Mobilenet class\n",
    "#### Architecture\n",
    "![title](ArchMobileNet.png)\n",
    "#### Normal Convolution and Depthwise convolution\n",
    "![title](MobileNetConvs.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MobileNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MobileNet, self).__init__()\n",
    "\n",
    "        # Normal convolution block followed by Batchnorm (CONV_3x3-->BN-->Relu)\n",
    "        def conv_bn(inp, oup, stride):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        # Depthwise convolution block (CONV_BLK_3x3-->BN-->Relu-->CONV_1x1-->BN-->Relu)\n",
    "        def conv_dw(inp, oup, stride):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n",
    "                nn.BatchNorm2d(inp),\n",
    "                nn.ReLU(inplace=True),\n",
    "    \n",
    "                nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            conv_bn(  3,  32, 2), \n",
    "            conv_dw( 32,  64, 1),\n",
    "            conv_dw( 64, 128, 2),\n",
    "            conv_dw(128, 128, 1),\n",
    "            conv_dw(128, 256, 2),\n",
    "            conv_dw(256, 256, 1),\n",
    "            conv_dw(256, 512, 2),\n",
    "            conv_dw(512, 512, 1),\n",
    "            conv_dw(512, 512, 1),\n",
    "            conv_dw(512, 512, 1),\n",
    "            conv_dw(512, 512, 1),\n",
    "            conv_dw(512, 512, 1),\n",
    "            conv_dw(512, 1024, 2),\n",
    "            conv_dw(1024, 1024, 1),\n",
    "            nn.AvgPool2d(7),\n",
    "        )\n",
    "        self.fc = nn.Linear(1024, 1000)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = x.view(-1, 1024)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize model and pass to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = MobileNet()\n",
    "#print(model)\n",
    "model = torch.nn.DataParallel(model).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define solver(SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), learning_rate,\n",
    "                                momentum=momentum,\n",
    "                                weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading specifics for ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data loading code\n",
    "traindir = os.path.join(IMAGENET_PATH, 'train')\n",
    "valdir = os.path.join(IMAGENET_PATH, 'val')\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# Operations that will be done on data\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.ImageFolder(traindir, transforms.Compose([\n",
    "            transforms.RandomSizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])),\n",
    "        batch_size=batch_size, shuffle=True,\n",
    "        num_workers=workers, pin_memory=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    datasets.ImageFolder(valdir, transforms.Compose([\n",
    "        transforms.Scale(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])),\n",
    "    batch_size=batch_size, shuffle=False,\n",
    "    num_workers=workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        target = target.cuda(async=True)\n",
    "        input_var = torch.autograd.Variable(input)\n",
    "        target_var = torch.autograd.Variable(target)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        top1.update(prec1[0], input.size(0))\n",
    "        top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1, top5=top5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/20019]\tTime 34.403 (34.403)\tData 10.950 (10.950)\tLoss 6.9082 (6.9082)\tPrec@1 0.000 (0.000)\tPrec@5 1.562 (1.562)\n",
      "Epoch: [0][100/20019]\tTime 4.356 (4.667)\tData 0.000 (0.109)\tLoss 6.8912 (6.9443)\tPrec@1 0.000 (0.248)\tPrec@5 0.000 (0.789)\n",
      "Epoch: [0][200/20019]\tTime 4.366 (4.517)\tData 0.000 (0.055)\tLoss 6.7890 (6.9095)\tPrec@1 0.000 (0.218)\tPrec@5 0.000 (0.910)\n",
      "Epoch: [0][300/20019]\tTime 4.368 (4.467)\tData 0.000 (0.037)\tLoss 6.7216 (6.8856)\tPrec@1 0.000 (0.234)\tPrec@5 0.000 (0.960)\n",
      "Epoch: [0][400/20019]\tTime 4.360 (4.442)\tData 0.000 (0.028)\tLoss 6.7019 (6.8599)\tPrec@1 0.000 (0.238)\tPrec@5 3.125 (1.040)\n",
      "Epoch: [0][500/20019]\tTime 4.375 (4.427)\tData 0.000 (0.022)\tLoss 6.6054 (6.8356)\tPrec@1 1.562 (0.259)\tPrec@5 3.125 (1.110)\n",
      "Epoch: [0][600/20019]\tTime 4.595 (4.429)\tData 0.000 (0.019)\tLoss 6.6863 (6.8106)\tPrec@1 0.000 (0.294)\tPrec@5 0.000 (1.271)\n",
      "Epoch: [0][700/20019]\tTime 4.534 (4.450)\tData 0.000 (0.016)\tLoss 6.5692 (6.7816)\tPrec@1 1.562 (0.334)\tPrec@5 1.562 (1.406)\n",
      "Epoch: [0][800/20019]\tTime 4.635 (4.471)\tData 0.000 (0.014)\tLoss 6.5095 (6.7566)\tPrec@1 0.000 (0.371)\tPrec@5 4.688 (1.553)\n",
      "Epoch: [0][900/20019]\tTime 4.644 (4.485)\tData 0.000 (0.012)\tLoss 6.6984 (6.7319)\tPrec@1 1.562 (0.434)\tPrec@5 1.562 (1.726)\n",
      "Epoch: [0][1000/20019]\tTime 4.376 (4.485)\tData 0.000 (0.011)\tLoss 6.3875 (6.7065)\tPrec@1 3.125 (0.453)\tPrec@5 7.812 (1.851)\n",
      "Epoch: [0][1100/20019]\tTime 4.367 (4.475)\tData 0.000 (0.010)\tLoss 6.4439 (6.6825)\tPrec@1 0.000 (0.478)\tPrec@5 0.000 (1.953)\n",
      "Epoch: [0][1200/20019]\tTime 4.359 (4.466)\tData 0.000 (0.009)\tLoss 6.3204 (6.6570)\tPrec@1 1.562 (0.518)\tPrec@5 3.125 (2.148)\n",
      "Epoch: [0][1300/20019]\tTime 4.372 (4.458)\tData 0.000 (0.009)\tLoss 6.3717 (6.6330)\tPrec@1 3.125 (0.562)\tPrec@5 4.688 (2.301)\n",
      "Epoch: [0][1400/20019]\tTime 4.365 (4.451)\tData 0.000 (0.008)\tLoss 6.3095 (6.6101)\tPrec@1 1.562 (0.594)\tPrec@5 3.125 (2.437)\n",
      "Epoch: [0][1500/20019]\tTime 4.363 (4.446)\tData 0.000 (0.008)\tLoss 6.3689 (6.5874)\tPrec@1 0.000 (0.629)\tPrec@5 1.562 (2.567)\n",
      "Epoch: [0][1600/20019]\tTime 4.364 (4.441)\tData 0.000 (0.007)\tLoss 6.3526 (6.5653)\tPrec@1 1.562 (0.659)\tPrec@5 3.125 (2.695)\n",
      "Epoch: [0][1700/20019]\tTime 4.380 (4.436)\tData 0.000 (0.007)\tLoss 5.9794 (6.5428)\tPrec@1 1.562 (0.701)\tPrec@5 6.250 (2.841)\n",
      "Epoch: [0][1800/20019]\tTime 4.369 (4.432)\tData 0.000 (0.006)\tLoss 6.2040 (6.5214)\tPrec@1 0.000 (0.731)\tPrec@5 3.125 (2.990)\n",
      "Epoch: [0][1900/20019]\tTime 4.360 (4.429)\tData 0.000 (0.006)\tLoss 6.4773 (6.5008)\tPrec@1 1.562 (0.769)\tPrec@5 3.125 (3.127)\n",
      "Epoch: [0][2000/20019]\tTime 4.366 (4.426)\tData 0.000 (0.006)\tLoss 5.9757 (6.4814)\tPrec@1 0.000 (0.807)\tPrec@5 12.500 (3.284)\n",
      "Epoch: [0][2100/20019]\tTime 4.372 (4.423)\tData 0.000 (0.005)\tLoss 6.2347 (6.4605)\tPrec@1 0.000 (0.849)\tPrec@5 3.125 (3.454)\n",
      "Epoch: [0][2200/20019]\tTime 4.362 (4.420)\tData 0.000 (0.005)\tLoss 6.0627 (6.4413)\tPrec@1 0.000 (0.892)\tPrec@5 4.688 (3.604)\n",
      "Epoch: [0][2300/20019]\tTime 4.359 (4.418)\tData 0.000 (0.005)\tLoss 5.9436 (6.4221)\tPrec@1 0.000 (0.934)\tPrec@5 10.938 (3.777)\n",
      "Epoch: [0][2400/20019]\tTime 4.359 (4.416)\tData 0.000 (0.005)\tLoss 6.0675 (6.4029)\tPrec@1 1.562 (0.981)\tPrec@5 3.125 (3.948)\n",
      "Epoch: [0][2500/20019]\tTime 4.369 (4.414)\tData 0.000 (0.005)\tLoss 5.7453 (6.3828)\tPrec@1 3.125 (1.034)\tPrec@5 7.812 (4.127)\n",
      "Epoch: [0][2600/20019]\tTime 4.374 (4.412)\tData 0.000 (0.004)\tLoss 5.8878 (6.3651)\tPrec@1 6.250 (1.081)\tPrec@5 9.375 (4.295)\n",
      "Epoch: [0][2700/20019]\tTime 4.367 (4.410)\tData 0.000 (0.004)\tLoss 5.7183 (6.3474)\tPrec@1 0.000 (1.134)\tPrec@5 7.812 (4.455)\n",
      "Epoch: [0][2800/20019]\tTime 4.364 (4.408)\tData 0.000 (0.004)\tLoss 6.0743 (6.3299)\tPrec@1 1.562 (1.185)\tPrec@5 9.375 (4.608)\n",
      "Epoch: [0][2900/20019]\tTime 4.357 (4.407)\tData 0.000 (0.004)\tLoss 5.7144 (6.3117)\tPrec@1 1.562 (1.244)\tPrec@5 17.188 (4.786)\n",
      "Epoch: [0][3000/20019]\tTime 4.361 (4.406)\tData 0.000 (0.004)\tLoss 5.6590 (6.2954)\tPrec@1 1.562 (1.294)\tPrec@5 14.062 (4.950)\n",
      "Epoch: [0][3100/20019]\tTime 4.371 (4.404)\tData 0.000 (0.004)\tLoss 5.6845 (6.2798)\tPrec@1 0.000 (1.343)\tPrec@5 3.125 (5.088)\n",
      "Epoch: [0][3200/20019]\tTime 4.361 (4.403)\tData 0.000 (0.004)\tLoss 5.5398 (6.2633)\tPrec@1 9.375 (1.396)\tPrec@5 21.875 (5.246)\n",
      "Epoch: [0][3300/20019]\tTime 4.366 (4.402)\tData 0.000 (0.004)\tLoss 5.7468 (6.2474)\tPrec@1 4.688 (1.441)\tPrec@5 10.938 (5.421)\n",
      "Epoch: [0][3400/20019]\tTime 4.360 (4.401)\tData 0.000 (0.004)\tLoss 5.9619 (6.2325)\tPrec@1 3.125 (1.494)\tPrec@5 15.625 (5.583)\n",
      "Epoch: [0][3500/20019]\tTime 4.372 (4.400)\tData 0.000 (0.003)\tLoss 5.6735 (6.2167)\tPrec@1 3.125 (1.544)\tPrec@5 14.062 (5.742)\n",
      "Epoch: [0][3600/20019]\tTime 4.357 (4.399)\tData 0.000 (0.003)\tLoss 5.4315 (6.2015)\tPrec@1 7.812 (1.602)\tPrec@5 18.750 (5.913)\n",
      "Epoch: [0][3700/20019]\tTime 4.368 (4.398)\tData 0.000 (0.003)\tLoss 5.8067 (6.1873)\tPrec@1 1.562 (1.647)\tPrec@5 6.250 (6.066)\n",
      "Epoch: [0][3800/20019]\tTime 4.366 (4.397)\tData 0.000 (0.003)\tLoss 5.4000 (6.1730)\tPrec@1 3.125 (1.697)\tPrec@5 18.750 (6.223)\n",
      "Epoch: [0][3900/20019]\tTime 4.370 (4.396)\tData 0.000 (0.003)\tLoss 5.4516 (6.1595)\tPrec@1 7.812 (1.754)\tPrec@5 18.750 (6.380)\n",
      "Epoch: [0][4000/20019]\tTime 4.365 (4.396)\tData 0.000 (0.003)\tLoss 5.9905 (6.1456)\tPrec@1 1.562 (1.807)\tPrec@5 7.812 (6.552)\n",
      "Epoch: [0][4100/20019]\tTime 4.368 (4.395)\tData 0.000 (0.003)\tLoss 5.4041 (6.1327)\tPrec@1 9.375 (1.862)\tPrec@5 14.062 (6.694)\n",
      "Epoch: [0][4200/20019]\tTime 4.374 (4.394)\tData 0.000 (0.003)\tLoss 5.5774 (6.1192)\tPrec@1 1.562 (1.908)\tPrec@5 10.938 (6.838)\n",
      "Epoch: [0][4300/20019]\tTime 4.365 (4.393)\tData 0.000 (0.003)\tLoss 5.1545 (6.1059)\tPrec@1 6.250 (1.961)\tPrec@5 21.875 (6.999)\n",
      "Epoch: [0][4400/20019]\tTime 4.365 (4.393)\tData 0.000 (0.003)\tLoss 5.5714 (6.0937)\tPrec@1 6.250 (2.007)\tPrec@5 17.188 (7.158)\n",
      "Epoch: [0][4500/20019]\tTime 4.362 (4.392)\tData 0.000 (0.003)\tLoss 5.3999 (6.0816)\tPrec@1 6.250 (2.055)\tPrec@5 15.625 (7.304)\n",
      "Epoch: [0][4600/20019]\tTime 4.365 (4.392)\tData 0.000 (0.003)\tLoss 5.7391 (6.0697)\tPrec@1 3.125 (2.110)\tPrec@5 20.312 (7.456)\n",
      "Epoch: [0][4700/20019]\tTime 4.366 (4.391)\tData 0.000 (0.003)\tLoss 5.4748 (6.0576)\tPrec@1 1.562 (2.157)\tPrec@5 18.750 (7.617)\n",
      "Epoch: [0][4800/20019]\tTime 4.364 (4.391)\tData 0.000 (0.003)\tLoss 5.5825 (6.0462)\tPrec@1 3.125 (2.211)\tPrec@5 9.375 (7.758)\n",
      "Epoch: [0][4900/20019]\tTime 4.371 (4.390)\tData 0.000 (0.003)\tLoss 5.7233 (6.0342)\tPrec@1 3.125 (2.270)\tPrec@5 12.500 (7.913)\n",
      "Epoch: [0][5000/20019]\tTime 4.368 (4.390)\tData 0.000 (0.002)\tLoss 5.4547 (6.0231)\tPrec@1 4.688 (2.316)\tPrec@5 14.062 (8.049)\n",
      "Epoch: [0][5100/20019]\tTime 4.376 (4.389)\tData 0.000 (0.002)\tLoss 5.2450 (6.0115)\tPrec@1 3.125 (2.360)\tPrec@5 17.188 (8.206)\n",
      "Epoch: [0][5200/20019]\tTime 4.371 (4.389)\tData 0.000 (0.002)\tLoss 5.7159 (6.0005)\tPrec@1 3.125 (2.413)\tPrec@5 7.812 (8.341)\n",
      "Epoch: [0][5300/20019]\tTime 4.360 (4.388)\tData 0.000 (0.002)\tLoss 5.1422 (5.9897)\tPrec@1 6.250 (2.460)\tPrec@5 25.000 (8.470)\n",
      "Epoch: [0][5400/20019]\tTime 4.366 (4.388)\tData 0.000 (0.002)\tLoss 5.4951 (5.9789)\tPrec@1 7.812 (2.515)\tPrec@5 17.188 (8.610)\n",
      "Epoch: [0][5500/20019]\tTime 4.371 (4.387)\tData 0.000 (0.002)\tLoss 5.2260 (5.9680)\tPrec@1 3.125 (2.569)\tPrec@5 17.188 (8.748)\n",
      "Epoch: [0][5600/20019]\tTime 4.371 (4.387)\tData 0.000 (0.002)\tLoss 5.5724 (5.9579)\tPrec@1 4.688 (2.616)\tPrec@5 12.500 (8.884)\n",
      "Epoch: [0][5700/20019]\tTime 4.368 (4.387)\tData 0.000 (0.002)\tLoss 5.3289 (5.9478)\tPrec@1 6.250 (2.669)\tPrec@5 15.625 (9.024)\n",
      "Epoch: [0][5800/20019]\tTime 4.366 (4.386)\tData 0.000 (0.002)\tLoss 5.3449 (5.9375)\tPrec@1 6.250 (2.724)\tPrec@5 15.625 (9.163)\n",
      "Epoch: [0][5900/20019]\tTime 4.361 (4.386)\tData 0.000 (0.002)\tLoss 5.5599 (5.9276)\tPrec@1 1.562 (2.770)\tPrec@5 7.812 (9.302)\n",
      "Epoch: [0][6000/20019]\tTime 4.372 (4.386)\tData 0.000 (0.002)\tLoss 5.2833 (5.9172)\tPrec@1 10.938 (2.824)\tPrec@5 17.188 (9.446)\n",
      "Epoch: [0][6100/20019]\tTime 4.370 (4.385)\tData 0.000 (0.002)\tLoss 5.2360 (5.9072)\tPrec@1 4.688 (2.878)\tPrec@5 12.500 (9.587)\n",
      "Epoch: [0][6200/20019]\tTime 4.367 (4.385)\tData 0.000 (0.002)\tLoss 5.0036 (5.8975)\tPrec@1 6.250 (2.921)\tPrec@5 17.188 (9.713)\n",
      "Epoch: [0][6300/20019]\tTime 4.366 (4.385)\tData 0.000 (0.002)\tLoss 5.2153 (5.8882)\tPrec@1 9.375 (2.974)\tPrec@5 21.875 (9.841)\n",
      "Epoch: [0][6400/20019]\tTime 4.359 (4.384)\tData 0.000 (0.002)\tLoss 4.9442 (5.8787)\tPrec@1 12.500 (3.026)\tPrec@5 26.562 (9.972)\n",
      "Epoch: [0][6500/20019]\tTime 4.373 (4.384)\tData 0.000 (0.002)\tLoss 4.8095 (5.8695)\tPrec@1 9.375 (3.076)\tPrec@5 25.000 (10.101)\n",
      "Epoch: [0][6600/20019]\tTime 4.357 (4.384)\tData 0.000 (0.002)\tLoss 5.0569 (5.8600)\tPrec@1 7.812 (3.125)\tPrec@5 23.438 (10.242)\n",
      "Epoch: [0][6700/20019]\tTime 4.364 (4.384)\tData 0.000 (0.002)\tLoss 5.0070 (5.8516)\tPrec@1 10.938 (3.165)\tPrec@5 25.000 (10.356)\n",
      "Epoch: [0][6800/20019]\tTime 4.366 (4.383)\tData 0.000 (0.002)\tLoss 5.1684 (5.8422)\tPrec@1 6.250 (3.214)\tPrec@5 28.125 (10.496)\n",
      "Epoch: [0][6900/20019]\tTime 4.374 (4.383)\tData 0.000 (0.002)\tLoss 5.4643 (5.8337)\tPrec@1 3.125 (3.262)\tPrec@5 14.062 (10.621)\n",
      "Epoch: [0][7000/20019]\tTime 4.365 (4.383)\tData 0.000 (0.002)\tLoss 5.6196 (5.8252)\tPrec@1 4.688 (3.320)\tPrec@5 9.375 (10.748)\n",
      "Epoch: [0][7100/20019]\tTime 4.361 (4.382)\tData 0.000 (0.002)\tLoss 4.9726 (5.8166)\tPrec@1 4.688 (3.362)\tPrec@5 20.312 (10.862)\n",
      "Epoch: [0][7200/20019]\tTime 4.358 (4.382)\tData 0.000 (0.002)\tLoss 5.3746 (5.8085)\tPrec@1 3.125 (3.409)\tPrec@5 20.312 (10.986)\n",
      "Epoch: [0][7300/20019]\tTime 4.356 (4.382)\tData 0.000 (0.002)\tLoss 4.8642 (5.7999)\tPrec@1 9.375 (3.461)\tPrec@5 21.875 (11.108)\n",
      "Epoch: [0][7400/20019]\tTime 4.372 (4.382)\tData 0.000 (0.002)\tLoss 5.1715 (5.7918)\tPrec@1 3.125 (3.509)\tPrec@5 15.625 (11.223)\n",
      "Epoch: [0][7500/20019]\tTime 4.368 (4.382)\tData 0.000 (0.002)\tLoss 4.8204 (5.7834)\tPrec@1 9.375 (3.556)\tPrec@5 23.438 (11.341)\n",
      "Epoch: [0][7600/20019]\tTime 4.366 (4.381)\tData 0.000 (0.002)\tLoss 5.0445 (5.7749)\tPrec@1 9.375 (3.600)\tPrec@5 15.625 (11.457)\n",
      "Epoch: [0][7700/20019]\tTime 4.365 (4.381)\tData 0.000 (0.002)\tLoss 4.9252 (5.7668)\tPrec@1 9.375 (3.648)\tPrec@5 21.875 (11.579)\n",
      "Epoch: [0][7800/20019]\tTime 4.358 (4.381)\tData 0.000 (0.002)\tLoss 4.6918 (5.7585)\tPrec@1 10.938 (3.694)\tPrec@5 21.875 (11.709)\n",
      "Epoch: [0][7900/20019]\tTime 4.359 (4.381)\tData 0.000 (0.002)\tLoss 5.1793 (5.7498)\tPrec@1 1.562 (3.748)\tPrec@5 23.438 (11.840)\n",
      "Epoch: [0][8000/20019]\tTime 4.359 (4.381)\tData 0.000 (0.002)\tLoss 5.3788 (5.7419)\tPrec@1 3.125 (3.802)\tPrec@5 14.062 (11.958)\n",
      "Epoch: [0][8100/20019]\tTime 4.366 (4.380)\tData 0.000 (0.002)\tLoss 5.0868 (5.7345)\tPrec@1 10.938 (3.850)\tPrec@5 17.188 (12.069)\n",
      "Epoch: [0][8200/20019]\tTime 4.377 (4.380)\tData 0.000 (0.002)\tLoss 4.9145 (5.7272)\tPrec@1 9.375 (3.899)\tPrec@5 28.125 (12.194)\n",
      "Epoch: [0][8300/20019]\tTime 4.379 (4.380)\tData 0.000 (0.002)\tLoss 5.0787 (5.7196)\tPrec@1 4.688 (3.944)\tPrec@5 18.750 (12.313)\n",
      "Epoch: [0][8400/20019]\tTime 4.366 (4.380)\tData 0.000 (0.002)\tLoss 5.2600 (5.7115)\tPrec@1 4.688 (3.994)\tPrec@5 15.625 (12.433)\n",
      "Epoch: [0][8500/20019]\tTime 4.377 (4.380)\tData 0.000 (0.002)\tLoss 5.1162 (5.7043)\tPrec@1 6.250 (4.045)\tPrec@5 17.188 (12.551)\n",
      "Epoch: [0][8600/20019]\tTime 4.368 (4.380)\tData 0.000 (0.002)\tLoss 4.7732 (5.6966)\tPrec@1 10.938 (4.089)\tPrec@5 31.250 (12.661)\n",
      "Epoch: [0][8700/20019]\tTime 4.358 (4.379)\tData 0.000 (0.002)\tLoss 4.9169 (5.6895)\tPrec@1 6.250 (4.137)\tPrec@5 28.125 (12.775)\n",
      "Epoch: [0][8800/20019]\tTime 4.363 (4.379)\tData 0.000 (0.002)\tLoss 4.9960 (5.6820)\tPrec@1 9.375 (4.188)\tPrec@5 28.125 (12.886)\n",
      "Epoch: [0][8900/20019]\tTime 4.359 (4.379)\tData 0.000 (0.002)\tLoss 4.7126 (5.6744)\tPrec@1 9.375 (4.238)\tPrec@5 29.688 (13.008)\n",
      "Epoch: [0][9000/20019]\tTime 4.358 (4.379)\tData 0.000 (0.002)\tLoss 5.2813 (5.6667)\tPrec@1 1.562 (4.292)\tPrec@5 18.750 (13.133)\n",
      "Epoch: [0][9100/20019]\tTime 4.379 (4.379)\tData 0.000 (0.001)\tLoss 4.8286 (5.6597)\tPrec@1 7.812 (4.339)\tPrec@5 31.250 (13.232)\n",
      "Epoch: [0][9200/20019]\tTime 4.364 (4.379)\tData 0.000 (0.001)\tLoss 4.8403 (5.6525)\tPrec@1 9.375 (4.385)\tPrec@5 29.688 (13.343)\n",
      "Epoch: [0][9300/20019]\tTime 4.368 (4.378)\tData 0.000 (0.001)\tLoss 4.9636 (5.6458)\tPrec@1 10.938 (4.431)\tPrec@5 29.688 (13.447)\n",
      "Epoch: [0][9400/20019]\tTime 4.365 (4.378)\tData 0.000 (0.001)\tLoss 5.1068 (5.6384)\tPrec@1 6.250 (4.484)\tPrec@5 17.188 (13.561)\n",
      "Epoch: [0][9500/20019]\tTime 4.368 (4.378)\tData 0.000 (0.001)\tLoss 4.6893 (5.6309)\tPrec@1 9.375 (4.536)\tPrec@5 26.562 (13.679)\n",
      "Epoch: [0][9600/20019]\tTime 4.362 (4.378)\tData 0.000 (0.001)\tLoss 4.8325 (5.6239)\tPrec@1 12.500 (4.589)\tPrec@5 28.125 (13.790)\n",
      "Epoch: [0][9700/20019]\tTime 4.365 (4.378)\tData 0.000 (0.001)\tLoss 4.7739 (5.6167)\tPrec@1 4.688 (4.638)\tPrec@5 21.875 (13.904)\n",
      "Epoch: [0][9800/20019]\tTime 4.368 (4.378)\tData 0.000 (0.001)\tLoss 5.0191 (5.6100)\tPrec@1 10.938 (4.690)\tPrec@5 26.562 (14.013)\n",
      "Epoch: [0][9900/20019]\tTime 4.376 (4.378)\tData 0.000 (0.001)\tLoss 5.2371 (5.6035)\tPrec@1 3.125 (4.739)\tPrec@5 20.312 (14.116)\n",
      "Epoch: [0][10000/20019]\tTime 4.363 (4.378)\tData 0.000 (0.001)\tLoss 5.0095 (5.5968)\tPrec@1 10.938 (4.785)\tPrec@5 25.000 (14.225)\n",
      "Epoch: [0][10100/20019]\tTime 4.368 (4.377)\tData 0.000 (0.001)\tLoss 4.7679 (5.5896)\tPrec@1 7.812 (4.841)\tPrec@5 25.000 (14.340)\n",
      "Epoch: [0][10200/20019]\tTime 4.366 (4.377)\tData 0.000 (0.001)\tLoss 5.3883 (5.5830)\tPrec@1 10.938 (4.890)\tPrec@5 21.875 (14.448)\n",
      "Epoch: [0][10300/20019]\tTime 4.373 (4.377)\tData 0.000 (0.001)\tLoss 4.8951 (5.5763)\tPrec@1 10.938 (4.942)\tPrec@5 23.438 (14.560)\n",
      "Epoch: [0][10400/20019]\tTime 4.357 (4.377)\tData 0.000 (0.001)\tLoss 4.7013 (5.5698)\tPrec@1 9.375 (4.992)\tPrec@5 21.875 (14.660)\n",
      "Epoch: [0][10500/20019]\tTime 4.363 (4.377)\tData 0.000 (0.001)\tLoss 4.7454 (5.5635)\tPrec@1 10.938 (5.039)\tPrec@5 31.250 (14.766)\n",
      "Epoch: [0][10600/20019]\tTime 4.368 (4.377)\tData 0.000 (0.001)\tLoss 5.0387 (5.5571)\tPrec@1 9.375 (5.087)\tPrec@5 25.000 (14.875)\n",
      "Epoch: [0][10700/20019]\tTime 4.365 (4.377)\tData 0.000 (0.001)\tLoss 5.1434 (5.5505)\tPrec@1 6.250 (5.142)\tPrec@5 17.188 (14.986)\n",
      "Epoch: [0][10800/20019]\tTime 4.360 (4.377)\tData 0.000 (0.001)\tLoss 4.7959 (5.5440)\tPrec@1 12.500 (5.197)\tPrec@5 28.125 (15.094)\n",
      "Epoch: [0][10900/20019]\tTime 4.372 (4.377)\tData 0.000 (0.001)\tLoss 4.6029 (5.5383)\tPrec@1 10.938 (5.240)\tPrec@5 31.250 (15.187)\n",
      "Epoch: [0][11000/20019]\tTime 4.364 (4.376)\tData 0.000 (0.001)\tLoss 5.0592 (5.5315)\tPrec@1 4.688 (5.296)\tPrec@5 23.438 (15.305)\n",
      "Epoch: [0][11100/20019]\tTime 4.371 (4.376)\tData 0.000 (0.001)\tLoss 5.1488 (5.5253)\tPrec@1 6.250 (5.341)\tPrec@5 15.625 (15.400)\n",
      "Epoch: [0][11200/20019]\tTime 4.375 (4.376)\tData 0.000 (0.001)\tLoss 5.1694 (5.5189)\tPrec@1 7.812 (5.389)\tPrec@5 15.625 (15.506)\n",
      "Epoch: [0][11300/20019]\tTime 4.375 (4.376)\tData 0.000 (0.001)\tLoss 4.5076 (5.5129)\tPrec@1 17.188 (5.435)\tPrec@5 35.938 (15.603)\n",
      "Epoch: [0][11400/20019]\tTime 4.361 (4.376)\tData 0.000 (0.001)\tLoss 4.9145 (5.5067)\tPrec@1 6.250 (5.486)\tPrec@5 21.875 (15.709)\n",
      "Epoch: [0][11500/20019]\tTime 4.367 (4.376)\tData 0.000 (0.001)\tLoss 4.9607 (5.5004)\tPrec@1 9.375 (5.530)\tPrec@5 21.875 (15.814)\n",
      "Epoch: [0][11600/20019]\tTime 4.362 (4.376)\tData 0.000 (0.001)\tLoss 4.4460 (5.4944)\tPrec@1 12.500 (5.578)\tPrec@5 29.688 (15.916)\n",
      "Epoch: [0][11700/20019]\tTime 4.366 (4.376)\tData 0.000 (0.001)\tLoss 5.0423 (5.4885)\tPrec@1 10.938 (5.626)\tPrec@5 25.000 (16.021)\n",
      "Epoch: [0][11800/20019]\tTime 4.362 (4.376)\tData 0.000 (0.001)\tLoss 4.9400 (5.4822)\tPrec@1 4.688 (5.674)\tPrec@5 26.562 (16.131)\n",
      "Epoch: [0][11900/20019]\tTime 4.366 (4.376)\tData 0.000 (0.001)\tLoss 5.0372 (5.4761)\tPrec@1 12.500 (5.724)\tPrec@5 28.125 (16.238)\n",
      "Epoch: [0][12000/20019]\tTime 4.375 (4.375)\tData 0.000 (0.001)\tLoss 4.3840 (5.4699)\tPrec@1 9.375 (5.770)\tPrec@5 37.500 (16.342)\n",
      "Epoch: [0][12100/20019]\tTime 4.367 (4.375)\tData 0.000 (0.001)\tLoss 5.4708 (5.4645)\tPrec@1 6.250 (5.808)\tPrec@5 17.188 (16.423)\n",
      "Epoch: [0][12200/20019]\tTime 4.356 (4.375)\tData 0.000 (0.001)\tLoss 4.7362 (5.4586)\tPrec@1 15.625 (5.852)\tPrec@5 32.812 (16.519)\n",
      "Epoch: [0][12300/20019]\tTime 4.368 (4.375)\tData 0.000 (0.001)\tLoss 4.7265 (5.4528)\tPrec@1 12.500 (5.895)\tPrec@5 28.125 (16.621)\n",
      "Epoch: [0][12400/20019]\tTime 4.375 (4.375)\tData 0.000 (0.001)\tLoss 4.6798 (5.4469)\tPrec@1 9.375 (5.939)\tPrec@5 31.250 (16.722)\n",
      "Epoch: [0][12500/20019]\tTime 4.368 (4.375)\tData 0.000 (0.001)\tLoss 4.9625 (5.4414)\tPrec@1 10.938 (5.988)\tPrec@5 20.312 (16.820)\n",
      "Epoch: [0][12600/20019]\tTime 4.361 (4.375)\tData 0.000 (0.001)\tLoss 4.7840 (5.4357)\tPrec@1 10.938 (6.037)\tPrec@5 34.375 (16.915)\n",
      "Epoch: [0][12700/20019]\tTime 4.373 (4.375)\tData 0.000 (0.001)\tLoss 4.6963 (5.4298)\tPrec@1 14.062 (6.086)\tPrec@5 31.250 (17.018)\n",
      "Epoch: [0][12800/20019]\tTime 4.364 (4.375)\tData 0.000 (0.001)\tLoss 4.9584 (5.4242)\tPrec@1 6.250 (6.132)\tPrec@5 26.562 (17.113)\n",
      "Epoch: [0][12900/20019]\tTime 4.366 (4.375)\tData 0.000 (0.001)\tLoss 4.8329 (5.4183)\tPrec@1 10.938 (6.185)\tPrec@5 26.562 (17.212)\n",
      "Epoch: [0][13000/20019]\tTime 4.365 (4.375)\tData 0.000 (0.001)\tLoss 4.6083 (5.4129)\tPrec@1 12.500 (6.230)\tPrec@5 28.125 (17.304)\n",
      "Epoch: [0][13100/20019]\tTime 4.373 (4.375)\tData 0.000 (0.001)\tLoss 4.8232 (5.4072)\tPrec@1 14.062 (6.281)\tPrec@5 25.000 (17.397)\n",
      "Epoch: [0][13200/20019]\tTime 4.371 (4.375)\tData 0.000 (0.001)\tLoss 4.6608 (5.4018)\tPrec@1 9.375 (6.324)\tPrec@5 26.562 (17.486)\n",
      "Epoch: [0][13300/20019]\tTime 4.361 (4.375)\tData 0.000 (0.001)\tLoss 4.8927 (5.3965)\tPrec@1 14.062 (6.368)\tPrec@5 23.438 (17.577)\n",
      "Epoch: [0][13400/20019]\tTime 4.365 (4.375)\tData 0.000 (0.001)\tLoss 4.5887 (5.3912)\tPrec@1 20.312 (6.408)\tPrec@5 25.000 (17.663)\n",
      "Epoch: [0][13500/20019]\tTime 4.375 (4.375)\tData 0.000 (0.001)\tLoss 4.7464 (5.3860)\tPrec@1 4.688 (6.449)\tPrec@5 20.312 (17.748)\n",
      "Epoch: [0][13600/20019]\tTime 4.362 (4.374)\tData 0.000 (0.001)\tLoss 4.8425 (5.3803)\tPrec@1 14.062 (6.497)\tPrec@5 28.125 (17.841)\n",
      "Epoch: [0][13700/20019]\tTime 4.364 (4.374)\tData 0.000 (0.001)\tLoss 4.6969 (5.3748)\tPrec@1 14.062 (6.544)\tPrec@5 26.562 (17.938)\n",
      "Epoch: [0][13800/20019]\tTime 4.375 (4.374)\tData 0.000 (0.001)\tLoss 4.9875 (5.3695)\tPrec@1 7.812 (6.590)\tPrec@5 17.188 (18.031)\n",
      "Epoch: [0][13900/20019]\tTime 4.368 (4.374)\tData 0.000 (0.001)\tLoss 4.2610 (5.3644)\tPrec@1 23.438 (6.634)\tPrec@5 34.375 (18.119)\n",
      "Epoch: [0][14000/20019]\tTime 4.357 (4.374)\tData 0.000 (0.001)\tLoss 5.0867 (5.3591)\tPrec@1 10.938 (6.679)\tPrec@5 25.000 (18.210)\n",
      "Epoch: [0][14100/20019]\tTime 4.360 (4.374)\tData 0.000 (0.001)\tLoss 4.3442 (5.3537)\tPrec@1 9.375 (6.723)\tPrec@5 35.938 (18.306)\n",
      "Epoch: [0][14200/20019]\tTime 4.377 (4.374)\tData 0.000 (0.001)\tLoss 4.6076 (5.3484)\tPrec@1 20.312 (6.770)\tPrec@5 34.375 (18.403)\n",
      "Epoch: [0][14300/20019]\tTime 4.370 (4.374)\tData 0.000 (0.001)\tLoss 4.7156 (5.3432)\tPrec@1 4.688 (6.813)\tPrec@5 25.000 (18.491)\n",
      "Epoch: [0][14400/20019]\tTime 4.359 (4.374)\tData 0.000 (0.001)\tLoss 4.2367 (5.3379)\tPrec@1 18.750 (6.862)\tPrec@5 35.938 (18.584)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0, epochs):\n",
    "        adjust_learning_rate(optimizer, epoch, learning_rate)\n",
    "\n",
    "        # train for one epoch\n",
    "        train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        prec1 = validate(val_loader, model, criterion)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "        is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': args.arch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, is_best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
