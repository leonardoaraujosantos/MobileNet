{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imagenet Training SqueezeNet\n",
    "\n",
    "### References\n",
    "* [Paper](https://arxiv.org/pdf/1602.07360.pdf)\n",
    "* [Pytorch(reference) implementation](https://github.com/pytorch/vision/blob/master/torchvision/models/squeezenet.py)\n",
    "* [Training Imagenet with Pytorch](https://github.com/pytorch/examples/tree/master/imagenet)\n",
    "* [Python3 Profiling](https://docs.python.org/3/library/profile.html)\n",
    "* [Issue with the pretrained models](https://github.com/DeepScale/SqueezeNet/issues/34)\n",
    "* [SqueezeNet Neural Style](https://github.com/lizeng614/SqueezeNet-Neural-Style-Pytorch)\n",
    "* [Converters](https://github.com/ysh329/deep-learning-model-convertor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torch.nn.init as init\n",
    "\n",
    "# Just some functions to average stuff, and save the model\n",
    "from utils_pytorch import *\n",
    "\n",
    "# Trainning parameters\n",
    "learning_rate = 0.04\n",
    "batch_size = 64\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "workers = 4\n",
    "print_freq = 100\n",
    "epochs = 2\n",
    "#IMAGENET_PATH ='/mnt/eulbh-nas01/qa_analitics/Apical_CNN_training_data/ImageNet/ILSVRC/Data/DET'\n",
    "IMAGENET_PATH = '/home/leoara01/work/IMAGENET/ILSVRC/Data/CLS-LOC'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fire Module\n",
    "![title](FireModule.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Fire(nn.Module):\n",
    "\n",
    "    def __init__(self, inplanes, squeeze_planes,\n",
    "                 expand1x1_planes, expand3x3_planes):\n",
    "        super(Fire, self).__init__()\n",
    "        self.inplanes = inplanes\n",
    "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
    "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
    "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n",
    "                                   kernel_size=1)\n",
    "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
    "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n",
    "                                   kernel_size=3, padding=1)\n",
    "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.squeeze_activation(self.squeeze(x))\n",
    "        # Concatenate results\n",
    "        return torch.cat([\n",
    "            self.expand1x1_activation(self.expand1x1(x)),\n",
    "            self.expand3x3_activation(self.expand3x3(x))\n",
    "        ], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture v1.1\n",
    "![title](SqueezeNetArch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SqueezeNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(SqueezeNet, self).__init__()        \n",
    "        self.num_classes = num_classes\n",
    "        self.features = nn.Sequential(\n",
    "                nn.Conv2d(3, 64, kernel_size=3, stride=2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "                Fire(64, 16, 64, 64),\n",
    "                Fire(128, 16, 64, 64),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "                Fire(128, 32, 128, 128),\n",
    "                Fire(256, 32, 128, 128),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "                Fire(256, 48, 192, 192),\n",
    "                Fire(384, 48, 192, 192),\n",
    "                Fire(384, 64, 256, 256),\n",
    "                Fire(512, 64, 256, 256),)\n",
    "        # Final convolution is initialized differently form the rest\n",
    "        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n",
    "        \n",
    "        # Add dropout, Relu and Average pool\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),final_conv,nn.ReLU(inplace=True),nn.AvgPool2d(13, stride=1))\n",
    "\n",
    "        # Initialize layers\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                if m is final_conv:\n",
    "                    init.normal(m.weight.data, mean=0.0, std=0.01)\n",
    "                else:\n",
    "                    init.kaiming_uniform(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x.view(x.size(0), self.num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize model and pass to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = SqueezeNet()\n",
    "#print(model)\n",
    "model = torch.nn.DataParallel(model).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define solver(SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), learning_rate,\n",
    "                                momentum=momentum,\n",
    "                                weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading specifics for ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data loading code\n",
    "traindir = os.path.join(IMAGENET_PATH, 'train')\n",
    "valdir = os.path.join(IMAGENET_PATH, 'val')\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# Operations that will be done on data\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.ImageFolder(traindir, transforms.Compose([\n",
    "            transforms.RandomSizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])),\n",
    "        batch_size=batch_size, shuffle=True,\n",
    "        num_workers=workers, pin_memory=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    datasets.ImageFolder(valdir, transforms.Compose([\n",
    "        transforms.Scale(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])),\n",
    "    batch_size=batch_size, shuffle=False,\n",
    "    num_workers=workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        target = target.cuda(async=True)\n",
    "        input_var = torch.autograd.Variable(input)\n",
    "        target_var = torch.autograd.Variable(target)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        top1.update(prec1[0], input.size(0))\n",
    "        top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1, top5=top5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/20019]\tTime 6.216 (6.216)\tData 3.115 (3.115)\tLoss 7.1151 (7.1151)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
      "Epoch: [0][100/20019]\tTime 0.682 (0.863)\tData 0.189 (0.322)\tLoss 6.9071 (6.9100)\tPrec@1 0.000 (0.139)\tPrec@5 1.562 (0.557)\n",
      "Epoch: [0][200/20019]\tTime 0.683 (0.846)\tData 0.000 (0.345)\tLoss 6.9078 (6.9089)\tPrec@1 0.000 (0.124)\tPrec@5 0.000 (0.544)\n",
      "Epoch: [0][300/20019]\tTime 0.681 (0.849)\tData 0.000 (0.349)\tLoss 6.9075 (6.9085)\tPrec@1 0.000 (0.104)\tPrec@5 0.000 (0.555)\n",
      "Epoch: [0][400/20019]\tTime 1.094 (0.843)\tData 0.864 (0.336)\tLoss 6.9096 (6.9084)\tPrec@1 0.000 (0.097)\tPrec@5 0.000 (0.526)\n",
      "Epoch: [0][500/20019]\tTime 0.683 (0.846)\tData 0.386 (0.354)\tLoss 6.9078 (6.9082)\tPrec@1 0.000 (0.094)\tPrec@5 0.000 (0.515)\n",
      "Epoch: [0][600/20019]\tTime 0.679 (0.846)\tData 0.000 (0.360)\tLoss 6.9074 (6.9082)\tPrec@1 0.000 (0.099)\tPrec@5 0.000 (0.528)\n",
      "Epoch: [0][700/20019]\tTime 0.687 (0.848)\tData 0.000 (0.363)\tLoss 6.9098 (6.9081)\tPrec@1 0.000 (0.091)\tPrec@5 0.000 (0.506)\n",
      "Epoch: [0][800/20019]\tTime 0.687 (0.845)\tData 0.000 (0.352)\tLoss 6.9074 (6.9081)\tPrec@1 0.000 (0.096)\tPrec@5 0.000 (0.521)\n",
      "Epoch: [0][900/20019]\tTime 0.752 (0.849)\tData 0.519 (0.355)\tLoss 6.9067 (6.9081)\tPrec@1 0.000 (0.095)\tPrec@5 0.000 (0.510)\n",
      "Epoch: [0][1000/20019]\tTime 0.935 (0.852)\tData 0.706 (0.359)\tLoss 6.9062 (6.9081)\tPrec@1 0.000 (0.092)\tPrec@5 0.000 (0.495)\n",
      "Epoch: [0][1100/20019]\tTime 0.988 (0.851)\tData 0.759 (0.358)\tLoss 6.9077 (6.9080)\tPrec@1 0.000 (0.102)\tPrec@5 0.000 (0.509)\n",
      "Epoch: [0][1200/20019]\tTime 0.674 (0.854)\tData 0.000 (0.363)\tLoss 6.9084 (6.9080)\tPrec@1 0.000 (0.099)\tPrec@5 0.000 (0.503)\n",
      "Epoch: [0][1300/20019]\tTime 1.042 (0.858)\tData 0.809 (0.369)\tLoss 6.9089 (6.9079)\tPrec@1 0.000 (0.106)\tPrec@5 0.000 (0.513)\n",
      "Epoch: [0][1400/20019]\tTime 0.680 (0.860)\tData 0.000 (0.371)\tLoss 6.9085 (6.9079)\tPrec@1 0.000 (0.107)\tPrec@5 0.000 (0.506)\n",
      "Epoch: [0][1500/20019]\tTime 0.683 (0.863)\tData 0.398 (0.375)\tLoss 6.9107 (6.9079)\tPrec@1 0.000 (0.107)\tPrec@5 0.000 (0.501)\n",
      "Epoch: [0][1600/20019]\tTime 0.684 (0.867)\tData 0.062 (0.380)\tLoss 6.9071 (6.9079)\tPrec@1 0.000 (0.106)\tPrec@5 0.000 (0.497)\n",
      "Epoch: [0][1700/20019]\tTime 0.683 (0.871)\tData 0.000 (0.385)\tLoss 6.9057 (6.9079)\tPrec@1 0.000 (0.106)\tPrec@5 1.562 (0.502)\n",
      "Epoch: [0][1800/20019]\tTime 0.687 (0.873)\tData 0.000 (0.388)\tLoss 6.9120 (6.9079)\tPrec@1 0.000 (0.105)\tPrec@5 0.000 (0.516)\n",
      "Epoch: [0][1900/20019]\tTime 0.692 (0.875)\tData 0.395 (0.394)\tLoss 6.9069 (6.9079)\tPrec@1 0.000 (0.106)\tPrec@5 0.000 (0.519)\n",
      "Epoch: [0][2000/20019]\tTime 0.686 (0.878)\tData 0.326 (0.396)\tLoss 6.9037 (6.9078)\tPrec@1 0.000 (0.107)\tPrec@5 1.562 (0.516)\n",
      "Epoch: [0][2100/20019]\tTime 0.688 (0.882)\tData 0.000 (0.400)\tLoss 6.9008 (6.9077)\tPrec@1 0.000 (0.106)\tPrec@5 0.000 (0.517)\n",
      "Epoch: [0][2200/20019]\tTime 0.848 (0.885)\tData 0.615 (0.405)\tLoss 6.9085 (6.9077)\tPrec@1 0.000 (0.104)\tPrec@5 1.562 (0.513)\n",
      "Epoch: [0][2300/20019]\tTime 2.269 (0.888)\tData 2.040 (0.410)\tLoss 6.9012 (6.9077)\tPrec@1 0.000 (0.105)\tPrec@5 0.000 (0.515)\n",
      "Epoch: [0][2400/20019]\tTime 0.680 (0.890)\tData 0.000 (0.413)\tLoss 6.9107 (6.9077)\tPrec@1 0.000 (0.103)\tPrec@5 0.000 (0.513)\n",
      "Epoch: [0][2500/20019]\tTime 0.925 (0.892)\tData 0.686 (0.416)\tLoss 6.9068 (6.9077)\tPrec@1 0.000 (0.101)\tPrec@5 1.562 (0.519)\n",
      "Epoch: [0][2600/20019]\tTime 0.678 (0.895)\tData 0.000 (0.421)\tLoss 6.9035 (6.9077)\tPrec@1 0.000 (0.102)\tPrec@5 1.562 (0.519)\n",
      "Epoch: [0][2700/20019]\tTime 0.681 (0.898)\tData 0.000 (0.426)\tLoss 6.9070 (6.9077)\tPrec@1 0.000 (0.102)\tPrec@5 0.000 (0.516)\n",
      "Epoch: [0][2800/20019]\tTime 0.686 (0.900)\tData 0.079 (0.430)\tLoss 6.9073 (6.9077)\tPrec@1 0.000 (0.103)\tPrec@5 0.000 (0.513)\n",
      "Epoch: [0][2900/20019]\tTime 0.681 (0.903)\tData 0.433 (0.434)\tLoss 6.9098 (6.9077)\tPrec@1 0.000 (0.103)\tPrec@5 0.000 (0.511)\n",
      "Epoch: [0][3000/20019]\tTime 0.991 (0.906)\tData 0.758 (0.438)\tLoss 6.9090 (6.9078)\tPrec@1 0.000 (0.103)\tPrec@5 0.000 (0.509)\n",
      "Epoch: [0][3100/20019]\tTime 1.216 (0.908)\tData 0.985 (0.442)\tLoss 6.9080 (6.9078)\tPrec@1 0.000 (0.102)\tPrec@5 0.000 (0.503)\n",
      "Epoch: [0][3200/20019]\tTime 0.688 (0.912)\tData 0.000 (0.447)\tLoss 6.9087 (6.9078)\tPrec@1 0.000 (0.103)\tPrec@5 0.000 (0.509)\n",
      "Epoch: [0][3300/20019]\tTime 0.717 (0.914)\tData 0.423 (0.450)\tLoss 6.9062 (6.9078)\tPrec@1 1.562 (0.104)\tPrec@5 1.562 (0.510)\n",
      "Epoch: [0][3400/20019]\tTime 0.688 (0.916)\tData 0.000 (0.453)\tLoss 6.9077 (6.9078)\tPrec@1 0.000 (0.103)\tPrec@5 1.562 (0.511)\n",
      "Epoch: [0][3500/20019]\tTime 0.682 (0.919)\tData 0.000 (0.454)\tLoss 6.9064 (6.9078)\tPrec@1 0.000 (0.102)\tPrec@5 3.125 (0.508)\n",
      "Epoch: [0][3600/20019]\tTime 2.529 (0.921)\tData 2.289 (0.457)\tLoss 6.9083 (6.9078)\tPrec@1 0.000 (0.101)\tPrec@5 0.000 (0.506)\n",
      "Epoch: [0][3700/20019]\tTime 0.679 (0.924)\tData 0.000 (0.460)\tLoss 6.9056 (6.9078)\tPrec@1 0.000 (0.100)\tPrec@5 0.000 (0.506)\n",
      "Epoch: [0][3800/20019]\tTime 1.233 (0.926)\tData 1.003 (0.462)\tLoss 6.9092 (6.9078)\tPrec@1 0.000 (0.099)\tPrec@5 0.000 (0.504)\n",
      "Epoch: [0][3900/20019]\tTime 0.691 (0.929)\tData 0.000 (0.465)\tLoss 6.9079 (6.9078)\tPrec@1 0.000 (0.100)\tPrec@5 0.000 (0.503)\n",
      "Epoch: [0][4000/20019]\tTime 0.685 (0.931)\tData 0.000 (0.466)\tLoss 6.9080 (6.9078)\tPrec@1 0.000 (0.101)\tPrec@5 0.000 (0.508)\n",
      "Epoch: [0][4100/20019]\tTime 1.043 (0.933)\tData 0.804 (0.469)\tLoss 6.9074 (6.9078)\tPrec@1 0.000 (0.101)\tPrec@5 0.000 (0.506)\n",
      "Epoch: [0][4200/20019]\tTime 0.685 (0.935)\tData 0.000 (0.470)\tLoss 6.9090 (6.9078)\tPrec@1 0.000 (0.102)\tPrec@5 0.000 (0.506)\n",
      "Epoch: [0][4300/20019]\tTime 1.484 (0.937)\tData 1.244 (0.473)\tLoss 6.9061 (6.9078)\tPrec@1 0.000 (0.102)\tPrec@5 0.000 (0.504)\n",
      "Epoch: [0][4400/20019]\tTime 0.706 (0.940)\tData 0.000 (0.477)\tLoss 6.9064 (6.9078)\tPrec@1 1.562 (0.102)\tPrec@5 1.562 (0.503)\n",
      "Epoch: [0][4500/20019]\tTime 0.690 (0.943)\tData 0.000 (0.480)\tLoss 6.9084 (6.9078)\tPrec@1 0.000 (0.103)\tPrec@5 0.000 (0.506)\n",
      "Epoch: [0][4600/20019]\tTime 0.690 (0.946)\tData 0.012 (0.484)\tLoss 6.9075 (6.9078)\tPrec@1 0.000 (0.103)\tPrec@5 1.562 (0.506)\n",
      "Epoch: [0][4700/20019]\tTime 0.899 (0.948)\tData 0.665 (0.487)\tLoss 6.9080 (6.9078)\tPrec@1 0.000 (0.103)\tPrec@5 0.000 (0.504)\n",
      "Epoch: [0][4800/20019]\tTime 0.703 (0.950)\tData 0.000 (0.490)\tLoss 6.9083 (6.9078)\tPrec@1 0.000 (0.103)\tPrec@5 0.000 (0.503)\n",
      "Epoch: [0][4900/20019]\tTime 1.871 (0.953)\tData 1.641 (0.495)\tLoss 6.9072 (6.9078)\tPrec@1 0.000 (0.102)\tPrec@5 1.562 (0.502)\n",
      "Epoch: [0][5000/20019]\tTime 2.127 (0.957)\tData 1.891 (0.500)\tLoss 6.9076 (6.9078)\tPrec@1 0.000 (0.101)\tPrec@5 0.000 (0.500)\n",
      "Epoch: [0][5100/20019]\tTime 0.682 (0.960)\tData 0.000 (0.504)\tLoss 6.9043 (6.9078)\tPrec@1 1.562 (0.101)\tPrec@5 3.125 (0.499)\n",
      "Epoch: [0][5200/20019]\tTime 0.897 (0.964)\tData 0.666 (0.509)\tLoss 6.9089 (6.9078)\tPrec@1 0.000 (0.101)\tPrec@5 0.000 (0.499)\n",
      "Epoch: [0][5300/20019]\tTime 0.681 (0.967)\tData 0.000 (0.513)\tLoss 6.9075 (6.9078)\tPrec@1 0.000 (0.101)\tPrec@5 1.562 (0.499)\n",
      "Epoch: [0][5400/20019]\tTime 0.683 (0.969)\tData 0.308 (0.515)\tLoss 6.9088 (6.9078)\tPrec@1 0.000 (0.101)\tPrec@5 0.000 (0.498)\n",
      "Epoch: [0][5500/20019]\tTime 0.687 (0.973)\tData 0.000 (0.518)\tLoss 6.9074 (6.9078)\tPrec@1 0.000 (0.101)\tPrec@5 0.000 (0.500)\n",
      "Epoch: [0][5600/20019]\tTime 0.684 (0.977)\tData 0.218 (0.523)\tLoss 6.9085 (6.9078)\tPrec@1 0.000 (0.102)\tPrec@5 0.000 (0.500)\n",
      "Epoch: [0][5700/20019]\tTime 0.683 (0.981)\tData 0.000 (0.526)\tLoss 6.9059 (6.9078)\tPrec@1 1.562 (0.103)\tPrec@5 1.562 (0.502)\n",
      "Epoch: [0][5800/20019]\tTime 2.456 (0.984)\tData 2.226 (0.531)\tLoss 6.9074 (6.9078)\tPrec@1 0.000 (0.102)\tPrec@5 0.000 (0.503)\n",
      "Epoch: [0][5900/20019]\tTime 0.681 (0.987)\tData 0.000 (0.534)\tLoss 6.9084 (6.9078)\tPrec@1 0.000 (0.101)\tPrec@5 0.000 (0.501)\n",
      "Epoch: [0][6000/20019]\tTime 5.212 (0.991)\tData 4.983 (0.539)\tLoss 6.9084 (6.9078)\tPrec@1 0.000 (0.101)\tPrec@5 0.000 (0.500)\n",
      "Epoch: [0][6100/20019]\tTime 0.685 (0.994)\tData 0.000 (0.542)\tLoss 6.9065 (6.9078)\tPrec@1 1.562 (0.101)\tPrec@5 1.562 (0.500)\n",
      "Epoch: [0][6200/20019]\tTime 0.686 (0.997)\tData 0.000 (0.547)\tLoss 6.9077 (6.9078)\tPrec@1 0.000 (0.101)\tPrec@5 0.000 (0.500)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0, epochs):\n",
    "        adjust_learning_rate(optimizer, epoch, learning_rate)\n",
    "\n",
    "        # train for one epoch\n",
    "        train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        prec1 = validate(val_loader, model, criterion)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "        is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': args.arch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, is_best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
